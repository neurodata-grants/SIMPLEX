\documentclass[simplex.tex]{subfiles}
% NO NEED TO INPUT PREAMBLES HERE
% packages are inherited; you can compile this on its own

\onlyinsubfile{
\title{NeuroData SIMPLEX Report: Subfile}
}

\begin{document}
\onlyinsubfile{
\maketitle
\thispagestyle{empty}

The following report documents the progress made by the labs of Randal~Burns and Joshua~T.~Vogelstein at Johns Hopkins University towards goals set by the DARPA SIMPLEX grant.

%%%% Table of Contents
\tableofcontents

%%%% Publications
\bibliographystyle{IEEEtran}
\begin{spacing}{0.5}
\section*{Publications, Presentations, and Talks}
%\vspace{-20pt}
\nocite{*}
{\footnotesize	\bibliography{simplex}}
\end{spacing}
%%%% End Publications
}

\subsection{LOL}

%We have proven the conditions under which LOL outperforms PCA.
%Specifically, PCA only outperforms LOL is we store enough eigenvectors
%such that the $d^\text{th}$ LOL outperforms PCA whenever the difference of the means
%and the first $d$ eigenvectors contain less information than the $(d + 1)^{\text{th}}$
%eigenvector.  The proof utilizes Chernoff divergences. More
%specifically, we can compute the Chernoff Information that one
%distribution has about another.  Thus, for a given low-dimensional
%projection, we can evaluate how far  $F_0$ and  $F_1$ are from one another, which
%determines the induced Bayes optimal error rate. The settings for which
%PCA outperforms LOL are quite pathological.


\end{document}
