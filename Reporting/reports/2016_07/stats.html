<!DOCTYPE html>
<html lang="en">

<head>

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="description" content="">
	<meta name="author" content="">

	<title>SIMPLEX April 2016 JHU</title>

	<!-- Bootstrap Core CSS -->
	<link href="css/bootstrap.min.css" rel="stylesheet">
	<!-- font awesome -->
	<link rel="stylesheet" href="font-awesome/css/font-awesome.css">

	<!-- Custom CSS -->
	<link href="css/main.css" rel="stylesheet">

	<!-- ocp icon -->
	<link rel="shortcut icon" href="ocp_main3.ico">

	<!-- bower:css -->
	<!-- <link rel="stylesheet" href="/content/themes/jhu_id/bower_components/normalize-css/normalize.css"> -->
	<link rel="stylesheet" href="../../fonts/gentona/gentona.css">
	<link rel="stylesheet" href="../../fonts/titling-gothic/titling-gothic.css">
	<link rel="stylesheet" href="../../fonts/quadon/quadon.css">
	<link rel="stylesheet" href="../../fonts/arnhem/arnhem.css">
	<!-- endbower -->

	<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <style>
        	div.md {
        		width:640px;
        		margin: auto;
        	}
      body {
        font-family: 'gentona';
      }
      h1, h2, h3, h4, h5, h6 {
        font-family: 'quadon';
        font-weight: 400;
        margin-bottom: 0;
      }
      
      .small {
        font-size: 0.8em;
      }

        </style>

    </head>

    <!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

    <body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

<script src="https://cdnjs.cloudflare.com/ajax/libs/markdown-it/5.0.2/markdown-it.js" charset="utf-8"></script>
<div class="md" id="md-raw">

[*Back*](./index.html)
# Statistical Science


## Time-Series


## Graphs


### Vertex Nomination via Seeded Graph Matching

We aim to identify vertices in one network that correspond to a particular vertex of interest in a second network. To this end, we consider a pair of networks for which there exists some notion of correspondence between vertices in the two networks, supposing that there is a particular vertex of interest (VOI) known to be in the first network, which has a corresponding vertex we would like to identify in the second network. We explore a principled methodology appropriate for situations in which the networks are too large for brute-force graph matching. Our methodology identifies vertices in the neighborhood of the VOI in the first network that have verifiable corresponding vertices in the second network. Leveraging these known correspondences as seeds, we match the induced subgraphs in each network generated by the neighborhoods of these verified seeds. We then rank the vertices of the second network in terms of the most likely matches to the original VOI. We demonstrate the practicality of our methodology through simulations and real data examples.



### Law of Large Graphs

We propose an algorithm to estimate the mean of a collection of graphs. Our methodology is motivated by the asymptotical distribution of the adjacency spectral embedding of random dot product graphs. To take advantage of the low-rank structure of the graphs, adjacency spectral embedding, a rank-reduction procedure, is applied to the element-wise MLE. We then give a closed form for asymptotic relative efficiency between our estimator and the element-wise MLE, which theoretically proves that our estimator has smaller variance with sufficiently large number of vertices while keeping to be asymptotically unbiased. These results are demonstrated by various simulations. Moreover, our estimator also outperforms element-wise MLE for the CoRR brain graphs, which shows our estimator is valid even when the data does not perfectly follow a SBM.

<div class="fig-container" id="fig:LLG_RE">
<img src="https://raw.githubusercontent.com/TangRunze/LLG/master/Draft/RE.PNG" style="width: 700px; margin: 0px 0 0 -30px;" />
Figure: Scaled average relative efficiency with different N and fixed M of 1000 Monte Carlo replicates. Colors denote the block membership associated with the edges we are averaging over. Dashed lines represent the 95% confidence interval. Solid line in black represents the theoretical value for scaled RE. Observe that they all converge to theoretical value as expected.
</div>

<div class="fig-container" id="fig:LLG_JHU">
<img src="https://raw.githubusercontent.com/TangRunze/LLG/master/Draft/JHU.png" style="width: 700px; margin: 0px 0 0 -30px;" />
Figure: Comparison of mean squared error between Abar (red) and Phat (blue) for JHU dataset while embedding the graphs into different dimensions with different size M of the subsamples. The dimension chosen by Zhu and Ghodsi is denoted in green (2nd elbow) and purple (3rd elbow). Dashed lines represent the 95% confidence interval. When M is small, Phat outperforms Abar with a flexible range of the embedding dimension including what Zhu and Ghodsi selects.
</div>

### Robust Law of Large Graphs

To estimate the mean of a collection of weighted graphs under a low rank random graph model (e.g. Stochastic Blockmodel) when observing contaminated graphs, we propose an estimator which not only inherits robustness from element-wise robust estimators but also has small variance due to application of a rank-reduction procedure. Under appropriate conditions, we prove that our estimator outperforms standard estimators via asymptotic relative efficiency. We illustrate our theory and methods by Monte Carlo simulation studies and experimental results.

<div class="fig-container" id="fig:RobustLLG_est_comp">
<img src="../../figs/RobustLLG_estimator_comparison.png" style="width: 700px; margin: 0px 0 0 -30px;" />
Figure: Comparison of Estimators. Our estimator wins according to asymptotic mean squared error by taking advantage of robust estimators and rank-reduction procedure.
</div>

<div class="fig-container" id="fig:RobustLLG_cluster">
<img src="../../figs/RobustLLG_cluster.png" style="width: 700px; margin: 0px 0 0 -30px;"/>
Figure: Plots for embedded latent positions under different circumstance. Without conatmination, out estimator (yellow) is slightly worse than element-wise MLE (green) for estimating the true latent position (black);
With contamination, out estimator (blue), which degrades slowly, outperforms the element-wise MLE (red).
</div>


### Optimization Theoretical Vertex Clustering


We propose a methodology to cluster the vertices in a graph through optimization approach. By balancing between estimation of the probability matrix and the approximate block structure, we obtain the estimated latent positions and the cluster memberships simultaneously. We demonstrate in simulation that our optimization approach outperforms adjacency spectral embedding. Moreover, it is as good as empirical bayes approach but runs much faster.


## Joint Embedding

We develop a procedure to jointly embed multiple graphs. Contrast to adjacency spectral embedding and laplacian eigenmap, joint embedding take adavantage of multiple graphs to get a more stable estimates of embeddings. The method identifies a linear subspace
spanned by rank one symmetric matrices and projects adjacency matrices of graphs into the subspace. The embedding coefficients can be
treated as features of graphs. We propose a random graph model which can be used to model multiple random graphs. It can be
shown that under the model our method produces estimates of parameters with small errors. 
<div class="fig-container" id="fig:joint-embedding">
<img src="../../figs/leig_je_acc.png"  style="width: 700px; margin: 0px 0 0 -30px;" />

Figure: Classification accuracy based on two embedding approaches are plotted. Two classes of SBM graphs are generated and a K-NN classifier is used to classify graphs based on their embeddings. Joint embedding can take advantage of multiple graphs and achieve perfect classification perfarmance.
</div>

## Matrices


### Randomer Forest

In his landmark paper on random forests, Leo Breiman derived an upper bound for generalization error of random forests which depends on only two factors: 1) correlation among decision trees and 2) strength of individual decision trees. Lower correlation and stronger individual trees both enhance performance of the decision forest classifier. We have recently initiated an extensive sensitivity analysis of RerF to two different hyperparameters in order to understand how they affect both correlation and strength of decision trees, as well as overall classification performance. One of these hyperparameters controls the average number of nonzeros in candidate random projections sampled at split nodes. The other specifies the number of random projections sampled at each node. Preliminary results on the Sparse Parity simulated dataset with n (number of samples) = 1000 and p (number of dimensions) = 25 suggest that both hyperparameters have a significant effect on classification performance, with mtry having a more pronounced effect (top left figure, link below). To our knowledge, most studies typically only try values of mtry in the range of 1 to p. Even in Breimanâ€™s analysis, he only tries values of mtry on the order of or less than p. However, our results suggest that the optimal value of mtry can be much larger than p. The top right figure plots out-of-bag error for individual trees as a function of average number of nonzeros of selected split projections for three different values of mtry. The larger spread of points and lower average OOB error as mtry increases suggests that larger values of mtry improve classification performance by both increasing strength of individual trees and decreasing correlation. We are now conducting this analysis on our suite of benchmark datasets. These efforts will hopefully aid us in understanding more deeply how these parameters affect classification performance and allow us to more efficiently tune these parameters on any given classification task.

Additionally, we recomputed performance profiles on the benchmark data under random scaling and random affine transformations. The new calculations apply more pronounced scaling factors in order to better demonstrate the effectiveness of passing the data to ranks before fitting RerF. The bottom figure shows that RerF (in green) performs better than RF (in blue) on the untransformed data, but performs worse than RF when the data is affine transformed. Rerf_r (in red), which passes to ranks before fitting RerF, performs better than RF on the affine transformed data.

<div class="fig-container" id="fig:q4_tt">
<img src="../../figs/Q4_TT.png" style="width: 700px; margin: 0px 0 0 -30px;" />
</div>


The merit of a particular classifier is determined by its probability of making an incorrect prediction, called the misclassification rate. It is widely known that three factors contribute to the misclassificat\
ion rate. One of these contributions is the intrinsic noise in the data and does not depend on the classifier. The other two contributions stem from the classification procedure, and are the bias and variance o\
f the classifier. Estimating the bias and variance for RF and RerF under various settings will help to elucidate the underlying forces contributing to overall classifier performance, and will aid in fine-tuning\
 the design of RerF. While several bias/variance decompositions have been proposed, we adopt that proposed by Kohavi and Wolpert (1996). Using their definitions, we estimated bias and variance of RF and RerF un\
der various settings.

<div class="fig-container" id="fig:rerfr-bias-variance">
<img src="../../figs/Sparse_parity_bias_variance.png" style="width: 700px; margin: 0px 0 0 -30px;" />
</div>

### Dependence Testing

Discovering the potential dependency between two data sets is one of the most fundamental tasks in data analysis, which is a challenging problem for modern real data with high-dimensionality, non-linearity, noise, etc.

We propose a multiscale graph correlation to test dependency between two data sets. By combining local graph information with distance correlation, our proposed test statistic is theoretically consistent, exhibits superior testing powers under various types of dependencies, is able to identify potential local relationships, is robust against outliers of the data, and can be efficiently computed.




### Discriminability

Many scientific, government, and corporate groups are collecting and processing massive datasets. To obtain optimal quantitative answers to any inquiry about data requires making decision about how the data should be processed. To this end, we have proposed and developed a formal definition of discriminability to guide data collection and processing. Specifically, discriminability is defined to be the probability that within subject distance to be smaller than across subject distance. We prove that discriminability provides an upper bound on Bayes predictive accuracy for any downstream inference task. Furthermore, we designed an estimator of discriminability which can be computed from test-retest data set, demonstrate that it is unbiased, and derive our estimators asymptotic distribution. We apply our discriminability methodology to neural image processing. We find the best threshold to convert the raw correlation matrices into binary graphs and find the best processing pipeline for fMRI.


[*Back*](./index.html)

</div>


    	<section class="copyright">
    		<div class="container">
    			<div class="row">
    				<div class="col-lg-12">
    					<p class="copyright">designed by <a href="https://github.com/neurodata/SIMPLEX/graphs/contributors" target="_blank">us</a>.   &copy; <a href="http://neurodata.io">neurodata.io</a>  2016</p>
    				</div>
    			</div>
    		</div>
    	</section>

    	<!-- jQuery -->
    	<script src="js/jquery.js"></script>

    	<!-- Bootstrap Core JavaScript -->
    	<script src="js/bootstrap.min.js"></script>

    	<!-- Scrolling Nav JavaScript -->
    	<script src="js/jquery.easing.min.js"></script>
    	<script src="js/main.js"></script>

        <script type="text/javascript">
            m = markdownit({
                html: true,
                linkify: true,
                typographer: true
            });
            document.getElementById('md-raw').innerHTML = m.render(
                document.getElementById('md-raw').innerHTML);
        </script>

    </body>

    </html>
